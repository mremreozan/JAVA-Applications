{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath='//p' data='<p class=\"class-1 class-2\">Hello Worl...'>, <Selector xpath='//p' data='<p id=\"p2\" class=\"class-2\">Choose \\n  ...'>, <Selector xpath='//p' data='<p class=\"class-2\">Thanks for Watchin...'>]\n",
      "****************\n",
      "['<p class=\"class-1 class-2\">Hello World!</p>', '<p id=\"p2\" class=\"class-2\">Choose \\n            <a href=\"http://datacamp.com\">DataCamp!</a>!\\n        </p>', '<p class=\"class-2\">Thanks for Watching!</p>']\n",
      "****************\n",
      "['Hello World!', 'Choose \\n            ', '!\\n        ', 'Thanks for Watching!']\n",
      "****************\n",
      "['Hello World!', 'Choose \\n            ', 'DataCamp!', '!\\n        ', 'Thanks for Watching!']\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "html='''\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose \n",
    "            <a href=\"http://datacamp.com\">DataCamp!</a>!\n",
    "        </p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Create a Selector selecting html as the HTML document\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath( '//p' )\n",
    "\n",
    "# Chain together xpath methods to select desired p element\n",
    "ps = sel.xpath( '//div' ).xpath( './p' ).extract()\n",
    "\n",
    "divsText = sel.xpath( '//p/text()' ).extract()\n",
    "\n",
    "divsTexts = sel.xpath( '//p//text()' ).extract()\n",
    "\n",
    "print(divs)\n",
    "print('****************')\n",
    "print(ps)\n",
    "print('****************')\n",
    "print(divsText)\n",
    "print('****************')\n",
    "print(divsTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1020 elements in the HTML document.\n",
      "You have found:  1020\n"
     ]
    }
   ],
   "source": [
    "url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get( url ).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector( text=html )\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print( \"There are 1020 elements in the HTML document.\")\n",
    "print( \"You have found: \", len( sel.xpath('//*') ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPath vs CSS Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html>body>span:nth-of-type(1) a'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'\n",
    "\n",
    "ps = sel.xpath( '//div' ).xpath( './p' )\n",
    "ps = sel.css( 'div' ).css( 'p' )\n",
    "\n",
    "# p element with id equal to p3, which includes the text of future generations of this p element.\n",
    "xpath = '//p[@id=\"p3\"]//text()'\n",
    "\n",
    "# p element with id equal to p3, which includes the text of future generations of this p element.\n",
    "css_locator = 'p#p3 ::text'\n",
    "\n",
    "# not have to include future generation of p\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "# not have to include future generation of p\n",
    "css_locator = 'p#p3::text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/courses'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/tracks/skill'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/tracks/career'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/instructors'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/pricing'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/groups/business'>, <Selector xpath=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' mobile-nav__item ')]/a/@href\" data='/groups/education'>]\n",
      "[<Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/courses'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/tracks/skill'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/tracks/career'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/instructors'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/pricing'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/groups/business'>, <Selector xpath='//li[@class=\"mobile-nav__item\"]/a/@href' data='/groups/education'>]\n",
      "['/courses', '/tracks/skill', '/tracks/career', '/instructors', '/pricing', '/groups/business', '/groups/education']\n",
      "['/courses', '/tracks/skill', '/tracks/career', '/instructors', '/pricing', '/groups/business', '/groups/education']\n"
     ]
    }
   ],
   "source": [
    "# Fill in the blank\n",
    "css_locator = 'li.mobile-nav__item > a::attr(href)'\n",
    "xpath = '//li[@class=\"mobile-nav__item\"]/a/@href'\n",
    "\n",
    "print(sel.css(css_locator))\n",
    "print(sel.xpath(xpath))\n",
    "print(sel.css(css_locator).extract())\n",
    "print(sel.xpath(xpath).extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath=\"descendant-or-self::symbol[@id = 'command']/text()\" data='\\n\\t\\t\\t'>, <Selector xpath=\"descendant-or-self::symbol[@id = 'command']/text()\" data='\\n\\t\\t'>]\n",
      "[<Selector xpath='//symbol[@id=\"command\"]/text()' data='\\n\\t\\t\\t'>, <Selector xpath='//symbol[@id=\"command\"]/text()' data='\\n\\t\\t'>]\n",
      "['\\n\\t\\t\\t', '\\n\\t\\t']\n",
      "['\\n\\t\\t\\t', '\\n\\t\\t']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//symbol[@id=\"command\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'symbol#command::text'\n",
    "\n",
    "print(sel.css(css_locator))\n",
    "print(sel.xpath(xpath))\n",
    "print(sel.css(css_locator).extract())\n",
    "print(sel.xpath(xpath).extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.datacamp.com/courses/all \n",
      " Data Science Courses: R & Python Analysis Tutorials | DataCamp\n"
     ]
    }
   ],
   "source": [
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "url_var = 'https://www.datacamp.com/courses/all'\n",
    "html = requests.get( url_var ).content\n",
    "response = HtmlResponse(url=url_var, body=html)\n",
    "\n",
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('//title/text()').extract_first()\n",
    "\n",
    "print( this_url, '\\n', this_title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text from the h4 element is: Introduction to R\n"
     ]
    }
   ],
   "source": [
    "# Select all desired div elements\n",
    "divs = response.css( 'div.course-block' )\n",
    "\n",
    "# Take the first div element\n",
    "first_div = divs[0]\n",
    "\n",
    "# Extract the text from the h4 element in first_div\n",
    "h4_text = first_div.css('h4::text').extract_first()\n",
    "\n",
    "# Print out the text\n",
    "print( \"The text from the h4 element is:\", h4_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The variable divs was a SelectorList, so the first element (first_div) in this list is a Selector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jonathan Cornelissen', 'Filip Schouwenaars', 'Gilles Inghelbrecht', 'Nick Carchedi']\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "    name = 'dcspider'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        # Create an extracted list of course author names\n",
    "        author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "        # Here we will just return the list of Authors\n",
    "        return author_names\n",
    "\n",
    "a= DCspider().parse(response)\n",
    "print(a[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:33 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2020-05-09 19:23:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0\n",
      "2020-05-09 19:23:34 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2020-05-09 19:23:34 [scrapy.extensions.telnet] INFO: Telnet Password: a420ca248b78d0e6\n",
      "2020-05-09 19:23:34 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-09 19:23:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-09 19:23:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-09 19:23:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-09 19:23:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-09 19:23:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-09 19:23:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-09 19:23:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2020-05-09 19:23:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:23:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time actually analyzing it. For this reason, it is critical to become familiar with the data cleaning process and all of the tools available to you along the way. This course provides a very basic introduction to cleaning data in R using the tidyr, dplyr, and stringr packages. After taking the course you'll be able to go from raw data to awesome insights as quickly and painlessly as possible!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this interactive tutorial, you will learn how to perform sophisticated dplyr techniques to carry out your data manipulation with R. First you will master the five verbs of R data manipulation with dplyr: select, mutate, filter, arrange and summarise. Next, you will learn how you can chain your dplyr operations using the pipe operator of the magrittr package. In the final section, the focus is on practicing how to subset your data using the group_by function, and how you can access data stored outside of R in a database. All said and done, you will be familiar with data manipulation tools and techniques that will allow you to efficiently manipulate data.\n",
      "The R data.table package is rapidly making its name as the number one choice for handling large datasets in R. This online data.table tutorial will bring you from data.table novice to expert in no time. Once you are introduced to the general form of a data.table query, you will learn the techniques to subset your data.table, how to update by reference and how you can use data.table’s set()-family in your workflow. The course finishes with more complex concepts such as indexing, keys and fast ordered joins. Upon completion of the course, you will be able to use data.table in R for a more efficient manipulation and analysis process. Enjoy!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This online machine learning course is perfect for those who have a solid basis in R and statistics, but are complete beginners with machine learning. After a broad overview of the discipline's most common techniques and applications, you'll gain more insight into the assessment and training of different machine learning models. The rest of the course is dedicated to a first reconnaissance with three of the most basic machine learning tasks: classification, regression and clustering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:23:38 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a general-purpose programming language that is becoming more and more popular for doing data science. Companies worldwide are using Python to harvest insights from their data and get a competitive edge. Unlike any other Python tutorial, this course focuses on Python specifically for data science. In our Intro to Python class, you will learn about powerful ways to store and manipulate data as well as cool data science tools to start your own analyses. Enter DataCamp’s online Python curriculum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn to create interactive graphs to display distributions, relationships, model fits, and more using ggvis.\n",
      "This follow-up course on Intermediate R does not cover new programming concepts. Instead, you will strengthen your knowledge of the topics in Intermediate R with a bunch of new and fun exercises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn how to write a data report quickly and effectively with the R Markdown package, and share your results with your friends, colleagues or the rest of the world.  Learn how you can author your own R Markdown reports, and how to automate the reporting process so that you have your own reproducible reports. By the end of the interactive data analysis reporting tutorial, you will be able to generate reports straight from your R code, documenting your work — and its results — as an HTML, pdf, slideshow or Microsoft Word document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:23:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-09 19:23:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5215,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2466139,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 9, 17, 23, 39, 734539),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 10,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2020, 5, 9, 17, 23, 36, 97960)}\n",
      "2020-05-09 19:23:39 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intermediate R course is the logical next stop on your journey in the R programming language. In this R training you will learn about conditional statements, loops and functions to power your own R scripts. Next, you can make your R code more efficient and readable using the apply functions. Finally, the utilities chapter gets you up to speed with regular expressions in the R programming language, data structure manipulations and times and dates. This R tutorial will allow you to learn R and take the next step in advancing your overall knowledge and capabilities while programming in R.\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "    name = 'dcdescr'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "    # First parse method\n",
    "    def parse( self, response ):\n",
    "        links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "        # Follow each of the extracted links\n",
    "        for link in links:\n",
    "            yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "    # Second parsing method\n",
    "    def parse_descr( self, response ):\n",
    "        # Extract course description\n",
    "        course_descr = response.css( 'p.course__description::text' ).extract()\n",
    "        # For now, just yield the course description\n",
    "        print( course_descr[0] )\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DCdescr)\n",
    "process.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:09:44 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2020-05-09 19:09:44 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0\n",
      "2020-05-09 19:09:44 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2020-05-09 19:09:45 [scrapy.extensions.telnet] INFO: Telnet Password: 6363987dfcaf38b7\n",
      "2020-05-09 19:09:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-09 19:09:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-09 19:09:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-09 19:09:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-09 19:09:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-09 19:09:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-09 19:09:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-09 19:09:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2020-05-09 19:09:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (failed 1 times): 503 Service Unavailable\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together']}\n",
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2020-05-09 19:09:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (failed 2 times): 503 Service Unavailable\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales']}\n",
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists']}\n",
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists'], 'Data Analysis in R, the data.table Way': ['Data.table novice', 'Data.table yeoman', 'Data.table expert']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:09:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-05-09 19:09:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists'], 'Data Analysis in R, the data.table Way': ['Data.table novice', 'Data.table yeoman', 'Data.table expert'], 'Intermediate R': ['Conditionals and Control Flow', 'Functions', 'Utilities', 'Loops', 'The apply family', 'Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities']}\n",
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists'], 'Data Analysis in R, the data.table Way': ['Data.table novice', 'Data.table yeoman', 'Data.table expert'], 'Intermediate R': ['Conditionals and Control Flow', 'Functions', 'Utilities', 'Loops', 'The apply family', 'Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities'], 'Introduction to Machine Learning': ['What is Machine Learning', 'Classification', 'Clustering', 'Performance measures', 'Regression', 'What is Machine Learning', 'Performance measures', 'Classification', 'Regression', 'Clustering']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:09:50 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (failed 3 times): 503 Service Unavailable\n",
      "2020-05-09 19:09:50 [scrapy.core.engine] DEBUG: Crawled (503) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists'], 'Data Analysis in R, the data.table Way': ['Data.table novice', 'Data.table yeoman', 'Data.table expert'], 'Intermediate R': ['Conditionals and Control Flow', 'Functions', 'Utilities', 'Loops', 'The apply family', 'Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities'], 'Introduction to Machine Learning': ['What is Machine Learning', 'Classification', 'Clustering', 'Performance measures', 'Regression', 'What is Machine Learning', 'Performance measures', 'Classification', 'Regression', 'Clustering'], 'Intermediate R - Practice': ['Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities']}\n",
      "{'Cleaning Data in R': ['Introduction and exploring raw data', 'Preparing data for analysis', 'Tidying data', 'Putting it all together', 'Introduction and exploring raw data', 'Tidying data', 'Preparing data for analysis', 'Putting it all together'], 'Data Manipulation in R with dplyr': ['Introduction to dplyr and tbls', 'Filter and arrange', 'Group_by and working with databases', 'Select and mutate', 'Summarize and the pipe operator', 'Introduction to dplyr and tbls', 'Select and mutate', 'Filter and arrange', 'Summarize and the pipe operator', 'Group_by and working with databases'], 'Data Visualization in R with ggvis': ['The Grammar of Graphics', 'Transformations', 'Customizing Axes, Legends, and Scales', 'Lines and Syntax', 'Interactivity and Layers', 'The Grammar of Graphics', 'Lines and Syntax', 'Transformations', 'Interactivity and Layers', 'Customizing Axes, Legends, and Scales'], 'Introduction to R': ['Intro to basics', 'Vectors', 'Matrices', 'Factors', 'Data frames', 'Lists'], 'Data Analysis in R, the data.table Way': ['Data.table novice', 'Data.table yeoman', 'Data.table expert'], 'Intermediate R': ['Conditionals and Control Flow', 'Functions', 'Utilities', 'Loops', 'The apply family', 'Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities'], 'Introduction to Machine Learning': ['What is Machine Learning', 'Classification', 'Clustering', 'Performance measures', 'Regression', 'What is Machine Learning', 'Performance measures', 'Classification', 'Regression', 'Clustering'], 'Intermediate R - Practice': ['Conditionals and Control Flow', 'Loops', 'Functions', 'The apply family', 'Utilities'], 'Intro to Python for Data Science': ['Python Basics', 'Functions and Packages', 'Python Lists', 'NumPy', 'Python Basics', 'Python Lists', 'Functions and Packages', 'NumPy']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 19:09:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown>: HTTP status code is not handled or not allowed\n",
      "2020-05-09 19:09:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-09 19:09:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 6113,\n",
      " 'downloader/request_count': 14,\n",
      " 'downloader/request_method_count/GET': 14,\n",
      " 'downloader/response_bytes': 2293404,\n",
      " 'downloader/response_count': 14,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'downloader/response_status_count/503': 3,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 9, 17, 9, 50, 906832),\n",
      " 'httperror/response_ignored_count': 2,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'httperror/response_ignored_status_count/503': 1,\n",
      " 'log_count/DEBUG': 15,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'retry/count': 2,\n",
      " 'retry/max_reached': 1,\n",
      " 'retry/reason_count/503 Service Unavailable': 2,\n",
      " 'scheduler/dequeued': 14,\n",
      " 'scheduler/dequeued/memory': 14,\n",
      " 'scheduler/enqueued': 14,\n",
      " 'scheduler/enqueued/memory': 14,\n",
      " 'start_time': datetime.datetime(2020, 5, 9, 17, 9, 46, 462517)}\n",
      "2020-05-09 19:09:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "        print(dc_dict)\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
