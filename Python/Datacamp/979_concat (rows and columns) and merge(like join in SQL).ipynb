{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "uber = pd.read_csv(\"Datasets/nyc_uber_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber1 = uber[uber['Date/Time']=='4/1/2014']\n",
    "uber2 = uber[uber['Date/Time']=='5/1/2014']\n",
    "uber3 = uber[uber['Date/Time']=='6/1/2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Date/Time, Lat, Lon, Base]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Date  Day  type_country  counts\n",
      "0  1/5/2015  289  Cases_Guinea  2776.0\n",
      "1  1/4/2015  288  Cases_Guinea  2775.0\n",
      "    type country\n",
      "0  Cases  Guinea\n",
      "1  Cases  Guinea\n",
      "         Date  Day  type_country  counts   type country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "# combine columns of two dataframe \n",
    "ebola = pd.read_csv('Datasets/ebola.csv')\n",
    "\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "print(ebola_melt.head(2))\n",
    "\n",
    "# create a new dataframe called ebola2\n",
    "index = range(ebola_melt.shape[0])\n",
    "columns = ['str_split', 'type', 'country']\n",
    "ebola2 = pd.DataFrame(index=index, columns=columns)\n",
    "ebola2.head()\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola2['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola2['type'] = ebola2.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola2['country'] = ebola2.str_split.str.get(1)\n",
    "\n",
    "# delete str_split column\n",
    "ebola2.drop(columns='str_split', inplace=True)\n",
    " \n",
    "print(ebola2.head(2))\n",
    "\n",
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt, ebola2], axis=1)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datasets\\\\airquality.csv', 'Datasets\\\\all_medalists.csv', 'Datasets\\\\austin_airport_departure_data_2015_july.csv', 'Datasets\\\\auto-mpg.csv', 'Datasets\\\\Automobile2.csv', 'Datasets\\\\automobiles.csv', 'Datasets\\\\bachelor.csv', 'Datasets\\\\BRICS.csv', 'Datasets\\\\cars.csv', 'Datasets\\\\digits.csv', 'Datasets\\\\dob_job_application_filings_subset.csv', 'Datasets\\\\ebola.csv', 'Datasets\\\\exchange.csv', 'Datasets\\\\gapminder.csv', 'Datasets\\\\gapminder2.csv', 'Datasets\\\\gapminder_tidy.csv', 'Datasets\\\\left_one.csv', 'Datasets\\\\left_table.csv', 'Datasets\\\\life_expectancy_at_birth.csv', 'Datasets\\\\nyc_uber_2014.csv', 'Datasets\\\\oil_price.csv', 'Datasets\\\\pennsylvania2012_turnout.csv', 'Datasets\\\\percent-bachelors-degrees-women-usa.csv', 'Datasets\\\\pittsburgh2013.csv', 'Datasets\\\\right2.csv', 'Datasets\\\\right_one.csv', 'Datasets\\\\right_table.csv', 'Datasets\\\\sales-feb-2015.csv', 'Datasets\\\\sales.csv', 'Datasets\\\\site.csv', 'Datasets\\\\sp500.csv', 'Datasets\\\\stocks.csv', 'Datasets\\\\table1.csv', 'Datasets\\\\table2.csv', 'Datasets\\\\tb.csv', 'Datasets\\\\tips.csv', 'Datasets\\\\tips1.csv', 'Datasets\\\\titanic.csv', 'Datasets\\\\titanic09.csv', 'Datasets\\\\titanic2.csv', 'Datasets\\\\tweets.csv', 'Datasets\\\\users.csv', 'Datasets\\\\visited.csv', 'Datasets\\\\weather_data_austin_2010.csv', 'Datasets\\\\winequality-red.csv', 'Datasets\\\\world_dev_ind.csv', 'Datasets\\\\world_ind_pop_data.csv', 'Datasets\\\\world_population.csv']\n",
      "     City  Edition     Sport Discipline             Athlete  NOC Gender  \\\n",
      "0  Athens     1896  Aquatics   Swimming       HAJOS, Alfred  HUN    Men   \n",
      "1  Athens     1896  Aquatics   Swimming    HERSCHMANN, Otto  AUT    Men   \n",
      "2  Athens     1896  Aquatics   Swimming   DRIVAS, Dimitrios  GRE    Men   \n",
      "3  Athens     1896  Aquatics   Swimming  MALOKINIS, Ioannis  GRE    Men   \n",
      "4  Athens     1896  Aquatics   Swimming  CHASAPIS, Spiridon  GRE    Men   \n",
      "\n",
      "                        Event Event_gender   Medal  \n",
      "0              100m freestyle            M    Gold  \n",
      "1              100m freestyle            M  Silver  \n",
      "2  100m freestyle for sailors            M  Bronze  \n",
      "3  100m freestyle for sailors            M    Gold  \n",
      "4  100m freestyle for sailors            M  Silver  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = 'Datasets/*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9d67769dae34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#  Read csv into a DataFrame: df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Append df to frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "allcsv = pd.concat(frames)\n",
    "    \n",
    "# Print the shape of uber\n",
    "print(allcsv.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(allcsv.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0_x   name    lat    long  Unnamed: 0_y  ident   site       dated\n",
      "0             0   DR-1 -49.85 -128.57             0    619   DR-1  1927-02-08\n",
      "1             0   DR-1 -49.85 -128.57             7    844   DR-1  1932-03-22\n",
      "2             1   DR-3 -47.15 -126.72             1    734   DR-3  1939-01-07\n",
      "3             1   DR-3 -47.15 -126.72             3    735   DR-3  1930-01-12\n",
      "4             1   DR-3 -47.15 -126.72             4    751   DR-3  1930-02-26\n",
      "5             1   DR-3 -47.15 -126.72             5    752   DR-3         NaN\n",
      "6             2  MSK-4 -48.87 -123.40             2    837  MSK-4  1932-01-14\n",
      "7             2  MSK-4 -48.87 -123.40             6    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "site = pd.read_csv('Datasets/site.csv')\n",
    "visited = pd.read_csv('Datasets/visited.csv')\n",
    "\n",
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Print o2o\n",
    "print(o2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-02 08:30:00</th>\n",
       "      <td>Hooli</td>\n",
       "      <td>Software</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02 21:00:00</th>\n",
       "      <td>Mediacore</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Company   Product  Units\n",
       "Date                                           \n",
       "2015-02-02 08:30:00      Hooli  Software      3\n",
       "2015-02-02 21:00:00  Mediacore  Hardware      9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('Datasets/sales-feb-2015.csv', index_col='Date', parse_dates=True)\n",
    "sales.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 19 entries, 2015-02-02 08:30:00 to 2015-02-26 09:00:00\n",
      "Data columns (total 9 columns):\n",
      "(Hardware, Company)    5 non-null object\n",
      "(Hardware, Product)    5 non-null object\n",
      "(Hardware, Units)      5 non-null float64\n",
      "(Software, Company)    9 non-null object\n",
      "(Software, Product)    9 non-null object\n",
      "(Software, Units)      9 non-null float64\n",
      "(Service, Company)     5 non-null object\n",
      "(Service, Product)     5 non-null object\n",
      "(Service, Units)       5 non-null float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 1.5+ KB\n",
      "None\n",
      "DatetimeIndex(['2015-02-02 08:30:00', '2015-02-02 21:00:00',\n",
      "               '2015-02-03 14:00:00', '2015-02-04 15:30:00',\n",
      "               '2015-02-04 22:00:00', '2015-02-05 02:00:00',\n",
      "               '2015-02-05 22:00:00', '2015-02-07 23:00:00',\n",
      "               '2015-02-09 09:00:00', '2015-02-09 13:00:00',\n",
      "               '2015-02-11 20:00:00', '2015-02-11 23:00:00',\n",
      "               '2015-02-16 12:00:00', '2015-02-19 11:00:00',\n",
      "               '2015-02-19 16:00:00', '2015-02-21 05:00:00',\n",
      "               '2015-02-21 20:30:00', '2015-02-25 00:30:00',\n",
      "               '2015-02-26 09:00:00'],\n",
      "              dtype='datetime64[ns]', name='Date', freq=None)\n",
      "                            Hardware         Software Service\n",
      "                             Company          Company Company\n",
      "Date                                                         \n",
      "2015-02-02 08:30:00              NaN            Hooli     NaN\n",
      "2015-02-02 21:00:00        Mediacore              NaN     NaN\n",
      "2015-02-03 14:00:00              NaN          Initech     NaN\n",
      "2015-02-04 15:30:00              NaN        Streeplex     NaN\n",
      "2015-02-04 22:00:00  Acme Coporation              NaN     NaN\n",
      "2015-02-05 02:00:00              NaN  Acme Coporation     NaN\n",
      "2015-02-05 22:00:00              NaN              NaN   Hooli\n",
      "2015-02-07 23:00:00  Acme Coporation              NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "hard = sales[sales.Product == 'Hardware']\n",
    "soft = sales[sales.Product == 'Software']\n",
    "serv = sales[sales.Product == 'Service']\n",
    "dataframes = [hard, soft, serv]\n",
    "\n",
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, keys=['Hardware','Software','Service'], axis=1)\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "print(february.index)\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['2015-2-2':'2015-2-8', idx[:,'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-21 19:13:21</th>\n",
       "      <td>Streeplex</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09 05:23:51</th>\n",
       "      <td>Streeplex</td>\n",
       "      <td>Service</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Company   Product  Units\n",
       "Date                                           \n",
       "2015-01-21 19:13:21  Streeplex  Hardware     11\n",
       "2015-01-09 05:23:51  Streeplex   Service      8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_2 = pd.read_csv('Datasets/sales/sales.csv', index_col='Date', parse_dates=True)\n",
    "sales_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 15 entries, (february, Acme Coporation) to (march, Streeplex)\n",
      "Data columns (total 1 columns):\n",
      "Units    15 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 286.0+ bytes\n",
      "None\n",
      "MultiIndex(levels=[['february', 'january', 'march'], ['Acme Coporation', 'Hooli', 'Initech', 'Mediacore', 'Streeplex']],\n",
      "           codes=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]],\n",
      "           names=[None, 'Company'])\n",
      "                          Units\n",
      "         Company               \n",
      "february Acme Coporation     34\n",
      "         Hooli               30\n",
      "         Initech             30\n",
      "         Mediacore           45\n",
      "         Streeplex           37\n",
      "january  Acme Coporation     76\n",
      "         Hooli               70\n",
      "         Initech             37\n",
      "         Mediacore           15\n",
      "         Streeplex           50\n",
      "march    Acme Coporation      5\n",
      "         Hooli               37\n",
      "         Initech             68\n",
      "         Mediacore           68\n",
      "         Streeplex           40\n",
      "                    Units\n",
      "         Company         \n",
      "february Mediacore     45\n",
      "january  Mediacore     15\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "jan = sales_2.loc['2015-01']\n",
    "feb = sales_2.loc['2015-02']\n",
    "mar = sales_2.loc['2015-03']\n",
    "\n",
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales_con = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales_con.info())\n",
    "print(sales_con.index)\n",
    "print(sales_con)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales_con.loc[idx[:, 'Mediacore'], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Total\n",
      "Country               \n",
      "United States   1052.0\n",
      "Soviet Union     584.0\n",
      "United Kingdom   505.0\n",
      "France           475.0\n",
      "Germany          454.0\n"
     ]
    }
   ],
   "source": [
    "bronze= pd.read_csv('Datasets/Summer Olympic medals/Bronze.csv', index_col='Country')\n",
    "bronze.drop('NOC', axis=1, inplace=True)\n",
    "silver= pd.read_csv('Datasets/Summer Olympic medals/Silver.csv', index_col='Country')\n",
    "silver.drop('NOC', axis=1, inplace=True)\n",
    "gold= pd.read_csv('Datasets/Summer Olympic medals/Gold.csv', index_col='Country')\n",
    "gold.drop('NOC', axis=1, inplace=True)\n",
    "print(bronze.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Bronze  Silver    Gold\n",
      "                 Total   Total   Total\n",
      "Country                               \n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n"
     ]
    }
   ],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['Bronze', 'Silver', 'Gold'], join='inner',axis=1)\n",
    "\n",
    "# Print medals\n",
    "print(medals.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
