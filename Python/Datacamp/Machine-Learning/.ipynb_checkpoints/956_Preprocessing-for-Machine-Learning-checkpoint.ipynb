{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 35)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer = pd.read_csv('Datasets/volunteer_opportunities.csv')\n",
    "volunteer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features are in the set after columns with at least 3 missing values are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 24)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer.dropna(thresh=3, axis=1, inplace=True)\n",
    "volunteer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to drop rows where the category_desc column values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "(617, 24)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Converting a column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opportunity_id          int64\n",
      "content_id              int64\n",
      "vol_requests            int64\n",
      "event_time              int64\n",
      "title                  object\n",
      "hits                    int64\n",
      "summary                object\n",
      "is_priority            object\n",
      "category_id           float64\n",
      "category_desc          object\n",
      "org_title              object\n",
      "org_content_id          int64\n",
      "addresses_count         int64\n",
      "locality               object\n",
      "region                 object\n",
      "postalcode            float64\n",
      "display_url            object\n",
      "recurrence_type        object\n",
      "hours                   int64\n",
      "created_date           object\n",
      "last_modified_date     object\n",
      "start_date_date        object\n",
      "end_date_date          object\n",
      "status                 object\n",
      "dtype: object\n",
      "0    NaN\n",
      "1    NaN\n",
      "2    NaN\n",
      "3    NaN\n",
      "4    NaN\n",
      "Name: is_priority, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes)\n",
    "\n",
    "# Print the head of the hits column\n",
    "print(volunteer[\"is_priority\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer.hits.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('Datasets/wine_types.csv')\n",
    "y = wine.iloc[:, 0].values\n",
    "X = wine.iloc[:, 1:].values\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# Create a KMeans instance with k clusters: model\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit model to samples\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see that the accuracy score is pretty low. Because some columns extremely high variance. Let's explore methods to improve this score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                0.600679\n",
       "Alcohol                             0.659062\n",
       "Malic acid                          1.248015\n",
       "Ash                                 0.075265\n",
       "Alcalinity of ash                  11.152686\n",
       "Magnesium                         203.989335\n",
       "Total phenols                       0.391690\n",
       "Flavanoids                          0.997719\n",
       "Nonflavanoid phenols                0.015489\n",
       "Proanthocyanins                     0.327595\n",
       "Color intensity                     5.374449\n",
       "Hue                                 0.052245\n",
       "OD280/OD315 of diluted wines        0.504086\n",
       "Proline                         99166.717355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Proline column has an extremely high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17231366191842012\n"
     ]
    }
   ],
   "source": [
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine.Proline.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler for continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine[['Ash', 'Alcalinity of ash', 'Magnesium']] = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "y = wine.iloc[:, 0].values\n",
    "X = wine.iloc[:, 1:].values\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create a KMeans instance with k clusters: model\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit model to samples\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The increase in accuracy is worth the extra step of scaling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Feature Engneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665 entries, 0 to 664\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   opportunity_id      665 non-null    int64  \n",
      " 1   content_id          665 non-null    int64  \n",
      " 2   vol_requests        665 non-null    int64  \n",
      " 3   event_time          665 non-null    int64  \n",
      " 4   title               665 non-null    object \n",
      " 5   hits                665 non-null    float64\n",
      " 6   summary             665 non-null    object \n",
      " 7   is_priority         62 non-null     object \n",
      " 8   category_id         617 non-null    float64\n",
      " 9   category_desc       617 non-null    object \n",
      " 10  org_title           665 non-null    object \n",
      " 11  org_content_id      665 non-null    int64  \n",
      " 12  addresses_count     665 non-null    int64  \n",
      " 13  locality            595 non-null    object \n",
      " 14  region              665 non-null    object \n",
      " 15  postalcode          659 non-null    float64\n",
      " 16  display_url         665 non-null    object \n",
      " 17  recurrence_type     665 non-null    object \n",
      " 18  hours               665 non-null    int64  \n",
      " 19  created_date        665 non-null    object \n",
      " 20  last_modified_date  665 non-null    object \n",
      " 21  start_date_date     665 non-null    object \n",
      " 22  end_date_date       665 non-null    object \n",
      " 23  status              665 non-null    object \n",
      "dtypes: float64(3), int64(7), object(14)\n",
      "memory usage: 124.8+ KB\n"
     ]
    }
   ],
   "source": [
    "volunteer.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title, created_date, category_desc. All three of these columns will require some feature engineering before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education  Emergency Preparedness  Environment  Health  \\\n",
      "0          0                       0            0       0   \n",
      "1          0                       0            0       0   \n",
      "2          0                       0            0       0   \n",
      "3          0                       0            0       0   \n",
      "4          0                       0            1       0   \n",
      "\n",
      "   Helping Neighbors in Need  Strengthening Communities  \n",
      "0                          0                          0  \n",
      "1                          0                          1  \n",
      "2                          0                          1  \n",
      "3                          0                          1  \n",
      "4                          0                          0  \n"
     ]
    }
   ],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encoder( 0 or 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = pd.read_json('Datasets/hiking.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prop_ID_value</th>\n",
       "      <th>Prop_ID_count</th>\n",
       "      <th>Name_value</th>\n",
       "      <th>Name_count</th>\n",
       "      <th>Location_value</th>\n",
       "      <th>Location_count</th>\n",
       "      <th>Park_Name_value</th>\n",
       "      <th>Park_Name_count</th>\n",
       "      <th>Length_value</th>\n",
       "      <th>Length_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Other_Details_value</th>\n",
       "      <th>Other_Details_count</th>\n",
       "      <th>Accessible_value</th>\n",
       "      <th>Accessible_count</th>\n",
       "      <th>Limited_Access_value</th>\n",
       "      <th>Limited_Access_count</th>\n",
       "      <th>lat_value</th>\n",
       "      <th>lat_count</th>\n",
       "      <th>lon_value</th>\n",
       "      <th>lon_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R013</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kazimiroff Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>Enter Park at Lincoln Road and Ocean Avenue en...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Van Cortlandt Park</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5 miles</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>32.0</td>\n",
       "      <td>N</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X092</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Blue Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>200 feet ahead of parking lot at the terminus ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>La Tourette Parks &amp; Golf Course</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5 miles</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>This is the Greenbelt&amp;rsquo;s longest marked t...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B073</td>\n",
       "      <td>4.0</td>\n",
       "      <td>John Muir Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>Willowbrook Park off Victory Boulevard</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Prospect Park</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75 miles</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Connects to Great Kills Park of the Gateway Na...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Clove Lakes Park Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>Richmond Road and St. Patrick's Place</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forest Park</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0 mile</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Take a step back in time and imagine Manhattan...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Orange Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>Page Avenue &amp; Eugene Street</td>\n",
       "      <td>1.0</td>\n",
       "      <td>High Rock Park</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.6 miles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Step back in time with a walk through Brooklyn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Midwood</td>\n",
       "      <td>1</td>\n",
       "      <td>Staten Island Boulevard</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cunningham Park</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.3 miles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>This gentle walk takes you through a forest of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R115</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Willowbrook Park White Trail</td>\n",
       "      <td>1</td>\n",
       "      <td>Enter the park at Van Cortlandt Park South and...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Arden Woods</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0 miles</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>This trail will lead you through a 2.4 mile ad...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prop_ID_value  Prop_ID_count                    Name_value  Name_count  \\\n",
       "0          R013            5.0              Kazimiroff Trail           1   \n",
       "1          X092            5.0                    Blue Trail           1   \n",
       "2          B073            4.0               John Muir Trail           1   \n",
       "3          Q015            3.0        Clove Lakes Park Trail           1   \n",
       "4          R088            2.0                  Orange Trail           1   \n",
       "5          X039            1.0                       Midwood           1   \n",
       "6          R115            1.0  Willowbrook Park White Trail           1   \n",
       "\n",
       "                                      Location_value  Location_count  \\\n",
       "0  Enter Park at Lincoln Road and Ocean Avenue en...             3.0   \n",
       "1  200 feet ahead of parking lot at the terminus ...             2.0   \n",
       "2             Willowbrook Park off Victory Boulevard             1.0   \n",
       "3              Richmond Road and St. Patrick's Place             1.0   \n",
       "4                        Page Avenue & Eugene Street             1.0   \n",
       "5                            Staten Island Boulevard             1.0   \n",
       "6  Enter the park at Van Cortlandt Park South and...             1.0   \n",
       "\n",
       "                   Park_Name_value  Park_Name_count Length_value  \\\n",
       "0               Van Cortlandt Park              5.0    1.5 miles   \n",
       "1  La Tourette Parks & Golf Course              5.0    0.5 miles   \n",
       "2                    Prospect Park              4.0   0.75 miles   \n",
       "3                      Forest Park              3.0     1.0 mile   \n",
       "4                   High Rock Park              2.0    7.6 miles   \n",
       "5                  Cunningham Park              1.0   12.3 miles   \n",
       "6                      Arden Woods              1.0    3.0 miles   \n",
       "\n",
       "   Length_count  ...                                Other_Details_value  \\\n",
       "0           3.0  ...                                                      \n",
       "1           3.0  ...  This is the Greenbelt&rsquo;s longest marked t...   \n",
       "2           3.0  ...  Connects to Great Kills Park of the Gateway Na...   \n",
       "3           2.0  ...  Take a step back in time and imagine Manhattan...   \n",
       "4           2.0  ...  Step back in time with a walk through Brooklyn...   \n",
       "5           2.0  ...  This gentle walk takes you through a forest of...   \n",
       "6           2.0  ...  This trail will lead you through a 2.4 mile ad...   \n",
       "\n",
       "   Other_Details_count Accessible_value  Accessible_count  \\\n",
       "0                  2.0                N              32.0   \n",
       "1                  2.0                Y               1.0   \n",
       "2                  2.0              NaN               NaN   \n",
       "3                  1.0              NaN               NaN   \n",
       "4                  1.0              NaN               NaN   \n",
       "5                  1.0              NaN               NaN   \n",
       "6                  1.0              NaN               NaN   \n",
       "\n",
       "  Limited_Access_value  Limited_Access_count lat_value  lat_count  lon_value  \\\n",
       "0                    N                  32.0       NaN        NaN        NaN   \n",
       "1                    Y                   1.0       NaN        NaN        NaN   \n",
       "2                  NaN                   NaN       NaN        NaN        NaN   \n",
       "3                  NaN                   NaN       NaN        NaN        NaN   \n",
       "4                  NaN                   NaN       NaN        NaN        NaN   \n",
       "5                  NaN                   NaN       NaN        NaN        NaN   \n",
       "6                  NaN                   NaN       NaN        NaN        NaN   \n",
       "\n",
       "   lon_count  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques = pd.DataFrame()\n",
    "for col in hiking:\n",
    "    col_uniques = pd.DataFrame({f'{col}_value': hiking[f'{col}'].value_counts().index,\n",
    "                                f'{col}_count': hiking[f'{col}'].value_counts().values})\n",
    "    uniques = pd.concat([uniques, col_uniques], axis = 1)\n",
    "\n",
    "print('Value counts:')\n",
    "uniques.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accessible  Accessible_enc\n",
      "0          Y               1\n",
      "1          N               0\n",
      "2          N               0\n",
      "3          N               0\n",
      "4          N               0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_date_converted  start_date_month\n",
      "0           2011-07-30                 7\n",
      "1           2011-02-01                 2\n",
      "2           2011-01-29                 1\n",
      "3           2011-02-14                 2\n",
      "4           2011-02-05                 2\n"
     ]
    }
   ],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking[\"Length\"] = hiking[\"Length\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row) )\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Selecting relevant features\n",
    "Now let's identify the redundant columns in the volunteer dataset and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
    "<br>\n",
    "<br>For example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain repeated information, so it would make sense to keep only one of the features.\n",
    "<br>\n",
    "<br>There are also features that have gone through the feature engineering process: columns like Education and Emergency Preparedness are a product of encoding the categorical variable category_desc, so category_desc itself is redundant now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   opportunity_id  content_id  event_time  \\\n",
      "0            4996       37004           0   \n",
      "\n",
      "                                               title   hits  \\\n",
      "0  Volunteers Needed For Rise Up & Stay Put! Home...  737.0   \n",
      "\n",
      "                                             summary is_priority  category_id  \\\n",
      "0  Building on successful events last summer and ...         NaN          NaN   \n",
      "\n",
      "                      org_title  org_content_id  ...  postalcode  \\\n",
      "0  Center For NYC Neighborhoods            4426  ...         NaN   \n",
      "\n",
      "           display_url recurrence_type hours  last_modified_date  \\\n",
      "0  /opportunities/4996         onetime     0        June 23 2011   \n",
      "\n",
      "  start_date_date end_date_date    status start_date_converted  \\\n",
      "0    July 30 2011  July 30 2011  approved           2011-07-30   \n",
      "\n",
      "  start_date_month  \n",
      "0                7  \n",
      "\n",
      "[1 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "print(volunteer_subset.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.328222</td>\n",
       "      <td>0.437776</td>\n",
       "      <td>-0.049643</td>\n",
       "      <td>0.517859</td>\n",
       "      <td>-0.209179</td>\n",
       "      <td>-0.719163</td>\n",
       "      <td>-0.847498</td>\n",
       "      <td>0.489109</td>\n",
       "      <td>-0.499130</td>\n",
       "      <td>0.265668</td>\n",
       "      <td>-0.617369</td>\n",
       "      <td>-0.788230</td>\n",
       "      <td>-0.569246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcohol</th>\n",
       "      <td>-0.328222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094397</td>\n",
       "      <td>0.211545</td>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.270798</td>\n",
       "      <td>0.289101</td>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>-0.071747</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>0.637325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malic acid</th>\n",
       "      <td>0.437776</td>\n",
       "      <td>0.094397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>-0.152643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ash</th>\n",
       "      <td>-0.049643</td>\n",
       "      <td>0.211545</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.238394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <td>0.517859</td>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>-0.416897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magnesium</th>\n",
       "      <td>-0.209179</td>\n",
       "      <td>0.270798</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.424006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total phenols</th>\n",
       "      <td>-0.719163</td>\n",
       "      <td>0.289101</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.431205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flavanoids</th>\n",
       "      <td>-0.847498</td>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>0.410494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <td>0.489109</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>-0.275675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <td>-0.499130</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>0.290203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color intensity</th>\n",
       "      <td>0.265668</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.348970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hue</th>\n",
       "      <td>-0.617369</td>\n",
       "      <td>-0.071747</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.173593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <td>-0.788230</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proline</th>\n",
       "      <td>-0.569246</td>\n",
       "      <td>0.637325</td>\n",
       "      <td>-0.152643</td>\n",
       "      <td>0.238394</td>\n",
       "      <td>-0.416897</td>\n",
       "      <td>0.424006</td>\n",
       "      <td>0.431205</td>\n",
       "      <td>0.410494</td>\n",
       "      <td>-0.275675</td>\n",
       "      <td>0.290203</td>\n",
       "      <td>0.348970</td>\n",
       "      <td>0.173593</td>\n",
       "      <td>0.254218</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Type   Alcohol  Malic acid       Ash  \\\n",
       "Type                          1.000000 -0.328222    0.437776 -0.049643   \n",
       "Alcohol                      -0.328222  1.000000    0.094397  0.211545   \n",
       "Malic acid                    0.437776  0.094397    1.000000  0.164045   \n",
       "Ash                          -0.049643  0.211545    0.164045  1.000000   \n",
       "Alcalinity of ash             0.517859 -0.310235    0.288500  0.443367   \n",
       "Magnesium                    -0.209179  0.270798   -0.054575  0.286587   \n",
       "Total phenols                -0.719163  0.289101   -0.335167  0.128980   \n",
       "Flavanoids                   -0.847498  0.236815   -0.411007  0.115077   \n",
       "Nonflavanoid phenols          0.489109 -0.155929    0.292977  0.186230   \n",
       "Proanthocyanins              -0.499130  0.136698   -0.220746  0.009652   \n",
       "Color intensity               0.265668  0.546364    0.248985  0.258887   \n",
       "Hue                          -0.617369 -0.071747   -0.561296 -0.074667   \n",
       "OD280/OD315 of diluted wines -0.788230  0.072343   -0.368710  0.003911   \n",
       "Proline                      -0.569246  0.637325   -0.152643  0.238394   \n",
       "\n",
       "                              Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "Type                                   0.517859  -0.209179      -0.719163   \n",
       "Alcohol                               -0.310235   0.270798       0.289101   \n",
       "Malic acid                             0.288500  -0.054575      -0.335167   \n",
       "Ash                                    0.443367   0.286587       0.128980   \n",
       "Alcalinity of ash                      1.000000  -0.083333      -0.321113   \n",
       "Magnesium                             -0.083333   1.000000       0.214401   \n",
       "Total phenols                         -0.321113   0.214401       1.000000   \n",
       "Flavanoids                            -0.351370   0.195784       0.864564   \n",
       "Nonflavanoid phenols                   0.361922  -0.256294      -0.449935   \n",
       "Proanthocyanins                       -0.197327   0.236441       0.612413   \n",
       "Color intensity                        0.018732   0.199950      -0.055136   \n",
       "Hue                                   -0.273955   0.055398       0.433681   \n",
       "OD280/OD315 of diluted wines          -0.276769   0.066004       0.699949   \n",
       "Proline                               -0.416897   0.424006       0.431205   \n",
       "\n",
       "                              Flavanoids  Nonflavanoid phenols  \\\n",
       "Type                           -0.847498              0.489109   \n",
       "Alcohol                         0.236815             -0.155929   \n",
       "Malic acid                     -0.411007              0.292977   \n",
       "Ash                             0.115077              0.186230   \n",
       "Alcalinity of ash              -0.351370              0.361922   \n",
       "Magnesium                       0.195784             -0.256294   \n",
       "Total phenols                   0.864564             -0.449935   \n",
       "Flavanoids                      1.000000             -0.537900   \n",
       "Nonflavanoid phenols           -0.537900              1.000000   \n",
       "Proanthocyanins                 0.652692             -0.365845   \n",
       "Color intensity                -0.172379              0.139057   \n",
       "Hue                             0.543479             -0.262640   \n",
       "OD280/OD315 of diluted wines    0.787194             -0.503270   \n",
       "Proline                         0.410494             -0.275675   \n",
       "\n",
       "                              Proanthocyanins  Color intensity       Hue  \\\n",
       "Type                                -0.499130         0.265668 -0.617369   \n",
       "Alcohol                              0.136698         0.546364 -0.071747   \n",
       "Malic acid                          -0.220746         0.248985 -0.561296   \n",
       "Ash                                  0.009652         0.258887 -0.074667   \n",
       "Alcalinity of ash                   -0.197327         0.018732 -0.273955   \n",
       "Magnesium                            0.236441         0.199950  0.055398   \n",
       "Total phenols                        0.612413        -0.055136  0.433681   \n",
       "Flavanoids                           0.652692        -0.172379  0.543479   \n",
       "Nonflavanoid phenols                -0.365845         0.139057 -0.262640   \n",
       "Proanthocyanins                      1.000000        -0.025250  0.295544   \n",
       "Color intensity                     -0.025250         1.000000 -0.521813   \n",
       "Hue                                  0.295544        -0.521813  1.000000   \n",
       "OD280/OD315 of diluted wines         0.519067        -0.428815  0.565468   \n",
       "Proline                              0.290203         0.348970  0.173593   \n",
       "\n",
       "                              OD280/OD315 of diluted wines   Proline  \n",
       "Type                                             -0.788230 -0.569246  \n",
       "Alcohol                                           0.072343  0.637325  \n",
       "Malic acid                                       -0.368710 -0.152643  \n",
       "Ash                                               0.003911  0.238394  \n",
       "Alcalinity of ash                                -0.276769 -0.416897  \n",
       "Magnesium                                         0.066004  0.424006  \n",
       "Total phenols                                     0.699949  0.431205  \n",
       "Flavanoids                                        0.787194  0.410494  \n",
       "Nonflavanoid phenols                             -0.503270 -0.275675  \n",
       "Proanthocyanins                                   0.519067  0.290203  \n",
       "Color intensity                                  -0.428815  0.348970  \n",
       "Hue                                               0.565468  0.173593  \n",
       "OD280/OD315 of diluted wines                      1.000000  0.254218  \n",
       "Proline                                           0.254218  1.000000  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for correlated features\n",
    "Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating.\n",
    "<br>Take a minute to look at the correlations. Identify a column where the correlation value is greater than 0.75 at least twice and store it in the to_drop variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Type   Alcohol  Malic acid       Ash  \\\n",
      "Type                          1.000000 -0.328222    0.437776 -0.049643   \n",
      "Alcohol                      -0.328222  1.000000    0.094397  0.211545   \n",
      "Malic acid                    0.437776  0.094397    1.000000  0.164045   \n",
      "Ash                          -0.049643  0.211545    0.164045  1.000000   \n",
      "Alcalinity of ash             0.517859 -0.310235    0.288500  0.443367   \n",
      "Magnesium                    -0.209179  0.270798   -0.054575  0.286587   \n",
      "Total phenols                -0.719163  0.289101   -0.335167  0.128980   \n",
      "Flavanoids                   -0.847498  0.236815   -0.411007  0.115077   \n",
      "Nonflavanoid phenols          0.489109 -0.155929    0.292977  0.186230   \n",
      "Proanthocyanins              -0.499130  0.136698   -0.220746  0.009652   \n",
      "Color intensity               0.265668  0.546364    0.248985  0.258887   \n",
      "Hue                          -0.617369 -0.071747   -0.561296 -0.074667   \n",
      "OD280/OD315 of diluted wines -0.788230  0.072343   -0.368710  0.003911   \n",
      "Proline                      -0.569246  0.637325   -0.152643  0.238394   \n",
      "\n",
      "                              Alcalinity of ash  Magnesium  Total phenols  \\\n",
      "Type                                   0.517859  -0.209179      -0.719163   \n",
      "Alcohol                               -0.310235   0.270798       0.289101   \n",
      "Malic acid                             0.288500  -0.054575      -0.335167   \n",
      "Ash                                    0.443367   0.286587       0.128980   \n",
      "Alcalinity of ash                      1.000000  -0.083333      -0.321113   \n",
      "Magnesium                             -0.083333   1.000000       0.214401   \n",
      "Total phenols                         -0.321113   0.214401       1.000000   \n",
      "Flavanoids                            -0.351370   0.195784       0.864564   \n",
      "Nonflavanoid phenols                   0.361922  -0.256294      -0.449935   \n",
      "Proanthocyanins                       -0.197327   0.236441       0.612413   \n",
      "Color intensity                        0.018732   0.199950      -0.055136   \n",
      "Hue                                   -0.273955   0.055398       0.433681   \n",
      "OD280/OD315 of diluted wines          -0.276769   0.066004       0.699949   \n",
      "Proline                               -0.416897   0.424006       0.431205   \n",
      "\n",
      "                              Flavanoids  Nonflavanoid phenols  \\\n",
      "Type                           -0.847498              0.489109   \n",
      "Alcohol                         0.236815             -0.155929   \n",
      "Malic acid                     -0.411007              0.292977   \n",
      "Ash                             0.115077              0.186230   \n",
      "Alcalinity of ash              -0.351370              0.361922   \n",
      "Magnesium                       0.195784             -0.256294   \n",
      "Total phenols                   0.864564             -0.449935   \n",
      "Flavanoids                      1.000000             -0.537900   \n",
      "Nonflavanoid phenols           -0.537900              1.000000   \n",
      "Proanthocyanins                 0.652692             -0.365845   \n",
      "Color intensity                -0.172379              0.139057   \n",
      "Hue                             0.543479             -0.262640   \n",
      "OD280/OD315 of diluted wines    0.787194             -0.503270   \n",
      "Proline                         0.410494             -0.275675   \n",
      "\n",
      "                              Proanthocyanins  Color intensity       Hue  \\\n",
      "Type                                -0.499130         0.265668 -0.617369   \n",
      "Alcohol                              0.136698         0.546364 -0.071747   \n",
      "Malic acid                          -0.220746         0.248985 -0.561296   \n",
      "Ash                                  0.009652         0.258887 -0.074667   \n",
      "Alcalinity of ash                   -0.197327         0.018732 -0.273955   \n",
      "Magnesium                            0.236441         0.199950  0.055398   \n",
      "Total phenols                        0.612413        -0.055136  0.433681   \n",
      "Flavanoids                           0.652692        -0.172379  0.543479   \n",
      "Nonflavanoid phenols                -0.365845         0.139057 -0.262640   \n",
      "Proanthocyanins                      1.000000        -0.025250  0.295544   \n",
      "Color intensity                     -0.025250         1.000000 -0.521813   \n",
      "Hue                                  0.295544        -0.521813  1.000000   \n",
      "OD280/OD315 of diluted wines         0.519067        -0.428815  0.565468   \n",
      "Proline                              0.290203         0.348970  0.173593   \n",
      "\n",
      "                              OD280/OD315 of diluted wines   Proline  \n",
      "Type                                             -0.788230 -0.569246  \n",
      "Alcohol                                           0.072343  0.637325  \n",
      "Malic acid                                       -0.368710 -0.152643  \n",
      "Ash                                               0.003911  0.238394  \n",
      "Alcalinity of ash                                -0.276769 -0.416897  \n",
      "Magnesium                                         0.066004  0.424006  \n",
      "Total phenols                                     0.699949  0.431205  \n",
      "Flavanoids                                        0.787194  0.410494  \n",
      "Nonflavanoid phenols                             -0.503270 -0.275675  \n",
      "Proanthocyanins                                   0.519067  0.290203  \n",
      "Color intensity                                  -0.428815  0.348970  \n",
      "Hue                                               0.565468  0.173593  \n",
      "OD280/OD315 of diluted wines                      1.000000  0.254218  \n",
      "Proline                                           0.254218  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
    "to_drop = \"Flavanoids\"\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping correlated features is often an iterative process, so you may need to try different combinations in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take the title text\n",
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "with open('Datasets/vocab.txt', 'r') as file:\n",
    "    text = file.readline()\n",
    "    while text != '':\n",
    "        text_list = text.split(':')\n",
    "        key = int(text_list[0])\n",
    "        value = text_list[1].split('\\n')[0]\n",
    "        vocab[key] = value\n",
    "        text = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201, 27, 590]\n"
     ]
    }
   ],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "#to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words.\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n",
      "(665, 836)\n"
     ]
    }
   ],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        try:\n",
    "            filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        except:\n",
    "            continue\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "print(len(filtered_words))\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "print(filtered_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50688767 0.16256262 0.11618906 0.07365662 0.05773053 0.03415481\n",
      " 0.0212432  0.01147232 0.00811959 0.0053799  0.00183666 0.00076702]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_X = wine.drop(\"Type\", axis=1)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the transformed X and the y labels into training and test sets\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(X_wine_train, y_wine_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(knn.score(X_wine_test, y_wine_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA is a decent choice for the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Case Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date       city state country     type    seconds  \\\n",
       "0  11/3/2011 19:21  woodville    wi      us  unknown  1209600.0   \n",
       "\n",
       "  length_of_time                                               desc  \\\n",
       "0        2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "\n",
       "     recorded         lat       long  \n",
       "0  12/12/2011  44.9530556 -92.291111  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo = pd.read_csv('Datasets/ufo_sightings_large.csv')\n",
    "ufo.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2011-11-03 19:21:00\n",
       "Name: date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo.date = pd.to_datetime(ufo.date)\n",
    "ufo.date.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "print(ufo[['length_of_time', 'state', 'type']].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo[ufo.length_of_time.notnull() & \n",
    "          ufo.state.notnull() & \n",
    "          ufo.type.notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numbers from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   minutes   length_of_time\n",
      "0      2.0          2 weeks\n",
      "1     30.0           30sec.\n",
      "2      NaN              NaN\n",
      "3      NaN  about 5 minutes\n",
      "4      2.0                2\n"
     ]
    }
   ],
   "source": [
    "def return_minutes(time_string):\n",
    "\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "    \n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, str(time_string))\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['minutes', 'length_of_time']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying features for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    3.156735e+10\n",
      "minutes    8.709933e+02\n",
      "dtype: float64\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/becode/snap/jupyter/common/lib/python3.7/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[[\"seconds\", \"minutes\"]].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = round(np.log(ufo['seconds']), 4)\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x : 1 if x=='us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo.type.unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo.type)\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2011-11-03 19:21:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "                 date  month  year\n",
      "0 2011-11-03 19:21:00     11  2011\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo.date.head(1))\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda x : x.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda x : x.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date', 'month', 'year']].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo[\"desc\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.desc.replace(np.nan, \"empty\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Red blinking objects similar to airplanes or s...\n",
      "Name: desc, dtype: object\n",
      "(4935, 6433)\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo[\"desc\"].head(1))\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo[\"desc\"])\n",
    "\n",
    "# Look at the number of columns this creates\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the ideal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = {}\n",
    "with open('Datasets/vocab2.txt', 'r') as file:\n",
    "    text = file.readline()\n",
    "    while text != '':\n",
    "        text_list = text.split(':')\n",
    "        key = int(text_list[0])\n",
    "        value = text_list[1].split('\\n')[0]\n",
    "        vocab[key] = value\n",
    "        text = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              seconds  seconds_log   minutes\n",
      "seconds      1.000000     0.164613 -0.008161\n",
      "seconds_log  0.164613     1.000000  0.110072\n",
      "minutes     -0.008161     0.110072  1.000000\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo[[\"seconds\", \"seconds_log\", \"minutes\"]].corr())\n",
    "\n",
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
    "\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_dropped = ufo_dropped.drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_dropped.seconds_log.replace(float('-inf'), -46.05, inplace=True)\n",
    "ufo_dropped.seconds_log.replace(float('inf'), 46.05, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ufo_dropped.drop(['country_enc'], axis=1)\n",
    "y = ufo_dropped.country_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 25 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   seconds_log  4935 non-null   float64\n",
      " 1   country_enc  4935 non-null   int64  \n",
      " 2   changing     4935 non-null   uint8  \n",
      " 3   chevron      4935 non-null   uint8  \n",
      " 4   cigar        4935 non-null   uint8  \n",
      " 5   circle       4935 non-null   uint8  \n",
      " 6   cone         4935 non-null   uint8  \n",
      " 7   cross        4935 non-null   uint8  \n",
      " 8   cylinder     4935 non-null   uint8  \n",
      " 9   diamond      4935 non-null   uint8  \n",
      " 10  disk         4935 non-null   uint8  \n",
      " 11  egg          4935 non-null   uint8  \n",
      " 12  fireball     4935 non-null   uint8  \n",
      " 13  flash        4935 non-null   uint8  \n",
      " 14  formation    4935 non-null   uint8  \n",
      " 15  light        4935 non-null   uint8  \n",
      " 16  other        4935 non-null   uint8  \n",
      " 17  oval         4935 non-null   uint8  \n",
      " 18  rectangle    4935 non-null   uint8  \n",
      " 19  sphere       4935 non-null   uint8  \n",
      " 20  teardrop     4935 non-null   uint8  \n",
      " 21  triangle     4935 non-null   uint8  \n",
      " 22  unknown      4935 non-null   uint8  \n",
      " 23  month        4935 non-null   int64  \n",
      " 24  year         4935 non-null   int64  \n",
      "dtypes: float64(1), int64(3), uint8(21)\n",
      "memory usage: 255.5 KB\n"
     ]
    }
   ],
   "source": [
    "ufo_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
      "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
      "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
      "       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
      "      dtype='object')\n",
      "0.7495948136142626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  This model performs pretty well. It seems like we've made pretty good feature selection choices here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22447325769854132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "nb = GaussianNB()\n",
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
