{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Idj3xHEmUL",
        "outputId": "812bde1d-bee5-4f79-865b-e1c3a02a308f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=09bab3b614ca88e8fa3eccd565f97901e6c24d401c1ec24a0e99afb835fde3c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3qDktLgP1w5",
        "outputId": "472c209d-e44b-4ad6-a696-7d681e8006c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "u5YI7gOyEzwE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext()"
      ],
      "metadata": {
        "id": "2LhW_kbrEvUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "# Print the Python version of SparkContext\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "# Print the master of SparkContext\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsm7VenSFEC5",
        "outputId": "8ffd4511-6fe8-410b-f298-8b8117b7c51e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The version of Spark Context in the PySpark shell is 3.5.0\n",
            "The Python version of Spark Context in the PySpark shell is 3.10\n",
            "The master of Spark Context in the PySpark shell is local[*]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "# Print my_list in the console\n",
        "print(\"Input list is\", my_list)\n",
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
        "# Print the result of the map function\n",
        "print(\"The squared numbers are\", squared_list_lambda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYqzSrbORWBd",
        "outputId": "98e6dba9-ee2c-4675-975b-0338250673fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
        "# Print my_list2 in the console\n",
        "print(\"Input list is:\", my_list2)\n",
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10 are:\", filtered_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngFClnziRYbM",
        "outputId": "6b943271-1266-406c-ff29-c664ccc9f76f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
            "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from a list of words\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
        "# Print out the type of the created object\n",
        "print(\"The type of RDD is\", type(RDD))\n",
        "# Check the number of partitions in fileRDD\n",
        "print(\"Number of partitions in RDD is\", RDD.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYXLZBQrRa8Z",
        "outputId": "90aa3c2f-560e-4e3a-e65f-c97883b53f6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of RDD is <class 'pyspark.rdd.RDD'>\n",
            "Number of partitions in RDD is 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"datasets/5000_points.txt\"\n",
        "# Create a fileRDD_part from file_path with 5 partitions\n",
        "fileRDD = sc.textFile(file_path)\n",
        "# Check the number of partitions in fileRDD\n",
        "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
        "# Create a fileRDD_part from file_path with 5 partitions\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
        "# Check the number of partitions in fileRDD_part\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRkkAFXORe-m",
        "outputId": "60f3ec62-73a0-4376-ec88-1ca79124e447"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in fileRDD is 2\n",
            "Number of partitions in fileRDD_part is 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that modifying the number of partitions may result in faster performance due to parallelization."
      ],
      "metadata": {
        "id": "8z5oLVo1Roml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReduceBykey and Collect"
      ],
      "metadata": {
        "id": "X-bS8l6GFJpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PairRDD Rdd with key value pairs\n",
        "Rdd = sc.parallelize([(\"a\",2), (\"b\",4), (\"c\",6), (\"b\",5), (\"a\", 4)])\n",
        "# Apply reduceByKey() operation on Rdd\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
        "# Iterate over the result and print the output\n",
        "for (letter, num) in Rdd_Reduced.collect():\n",
        "  print(\"Key {} has {} Counts\".format(letter, num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWdjnD1wFFTw",
        "outputId": "095fdc1b-a7a7-41fb-c1e1-d75e0e570c88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key b has 9 Counts\n",
            "Key c has 6 Counts\n",
            "Key a has 6 Counts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SortByKey and Collect"
      ],
      "metadata": {
        "id": "DaABLun5GE3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Rdd_Reduced = Rdd_Reduced.map(lambda x : (x[1], x[0]))\n",
        "# Sort the reduced RDD with the key by descending order\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
        "# Iterate over the result and retrieve all the elements of the RDD\n",
        "for (num, letter) in Rdd_Reduced_Sort.collect():\n",
        "  print(\"Key {} has {} Counts\".format(letter, num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4akfHNyFO7z",
        "outputId": "73280aec-da2f-425b-d6d2-715d058f1c6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key b has 9 Counts\n",
            "Key c has 6 Counts\n",
            "Key a has 6 Counts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### countByKey"
      ],
      "metadata": {
        "id": "mfykUC6OGEFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Rdd = sc.parallelize([(\"a\",2), (\"b\",4), (\"c\",6), (\"b\",5), (\"a\", 4)])\n",
        "# Count the unique keys\n",
        "total = Rdd.countByKey()\n",
        "# What is the type of total?\n",
        "print(\"The type of total is\", type(total))\n",
        "# Iterate over the total and print the output\n",
        "for k, v in total.items():\n",
        "  print(\"key\", k, \"has\", v, \"counts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAORlps_Nmwh",
        "outputId": "e9f0878d-a897-4e99-c906-5987c285f8cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of total is <class 'collections.defaultdict'>\n",
            "key a has 2 counts\n",
            "key b has 2 counts\n",
            "key c has 1 counts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculates the most common words from Complete Works of William Shakespeare."
      ],
      "metadata": {
        "id": "XBa6u-VTOw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"datasets/Complete_Shakespeare.txt\"\n",
        "# Create a baseRDD from the file path\n",
        "baseRDD = sc.textFile(file_path)\n",
        "# Split the lines of baseRDD into words\n",
        "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
        "# Count the total number of words\n",
        "print(\"Total number of words in splitRDD:\", splitRDD.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i_P4fxKN0mA",
        "outputId": "8e36f5f5-80cb-463b-dc71-ca86dddbbd47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in splitRDD: 128576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfF3KPcgQNB2",
        "outputId": "fab3f582-84cc-4de9-a70b-9b7e29951af0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "ERtbxQozQTQ0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
        "# Create a tuple of the word and 1\n",
        "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
        "# Count of the number of occurences of each word\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "kCO_zJDyO2P3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 words and their frequencies from the input RDD\n",
        "for word in resultRDD.take(10):\n",
        "\tprint(word)\n",
        "# Swap the keys and values from the input RDD\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "# Sort the keys in descending order\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
        "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "\tprint(\"{},{}\". format(word[1], word[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvhZzFTpQc1r",
        "outputId": "529bf4ac-aa6e-4be5-a94b-d4748c342ea0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Project', 9)\n",
            "('EBook', 1)\n",
            "('Shakespeare', 12)\n",
            "('use', 38)\n",
            "('anyone', 1)\n",
            "('anywhere', 1)\n",
            "('restrictions', 1)\n",
            "('whatsoever.', 1)\n",
            "('may', 162)\n",
            "('it,', 74)\n",
            "thou,650\n",
            "thy,574\n",
            "shall,393\n",
            "would,311\n",
            "good,295\n",
            "thee,286\n",
            "love,273\n",
            "Enter,269\n",
            "th',254\n",
            "make,225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVr4wf1TQiVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}